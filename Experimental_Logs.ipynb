{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experimental Logs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBJCpETph8uz"
      },
      "source": [
        "I tried out these for the Task-2 part->1.\n",
        "<br>\n",
        "At first i tried to train my model without applying any preprocessing technique and only resizing them.<br>\n",
        "The cross validation accuracy came out to be very low  ~ 10-25 % but the train accuracy went pretty high ~ 88-94%.\n",
        "<br>\n",
        "I realised that my model was overfitting very badly, it was mainly beacuse:-\n",
        "<br>\n",
        "<li>There was no much diference between any 2 images of a particular class.</li>\n",
        "<li>Secondly, there were many white spaces in the background of the image.</li>\n",
        "<br>\n",
        "So first i tried to train, after cropping the white spaces from the images.<br>\n",
        "But again the result were same the Train accuracy was too high as compared to the cross validation accuracy.\n",
        "<br>\n",
        "So to bring diversity in the dataset,i tried data augumentation,for that i used the datagenerator of keras which basically creates new dataset from the existing dataset.\n",
        "<br>\n",
        "After this , I tried to train a very deep CNN model with skip connections, with different parameters of data generator with different values and different learning rates, different number of epochs and batch size but again the model was overfitting.\n",
        "<br>\n",
        "I infered that the even after applying the preprocessing steps, as the data set was small so a very deep CNN model would overfit.\n",
        "<br>\n",
        "So i tried to reduce my parameters of the CNN model ,i reduced the number of layers in my model.\n",
        "I resized my images to (32,32) and converted them to grayscale.\n",
        "<br>\n",
        "After trying a lot of hyperparameter tuning.\n",
        "Finally i was successfull in building a model which performing pretty good.\n",
        "<br>\n",
        "But i noticed a problem with my model, the cross validation accuracy was  greater than my train accuracy in almost half the number of epochs.\n",
        "So again i tried to change the hyperparameters and parameters of datagenerator but it didn't helped out to solve the issue.\n",
        "<br>\n",
        "Then i tried dropout layer but still it didn't resolve the issue.\n",
        "<br>\n",
        "Finally i came up with the idea to apply BatchNormalisation after every conv layer so that every input to each layer would become standardized which would help in stabilizing the learning process.\n",
        "<br>\n",
        "And finally the issue was resolved and i found the best fit epoch which the 9th epoch and used the weights of this epochs for further tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzP5bOopwGuX"
      },
      "source": [
        "Task-2 part-2\n",
        "<br>\n",
        "I saved the checkpoints and compared between the two models in terms of accuracy, convergence time, precision score and recall score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2-8jtkPwnnX"
      },
      "source": [
        "Task-2 part-3\n",
        "<br>\n",
        "I saved the checkpoints and compared between the two models in terms of accuracy,precision score and recall score.\n",
        "<br>\n",
        "And analysed the effect of noise in the training data on the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBR4-b-3wzDc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}